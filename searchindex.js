Search.setIndex({"docnames": ["NLP-Enrichment", "analysis", "corpus_creation", "data_input_homogenisation", "intro", "nlp", "word_search_plot"], "filenames": ["NLP-Enrichment.ipynb", "analysis.md", "corpus_creation.md", "data_input_homogenisation.ipynb", "intro.md", "nlp.md", "word_search_plot.ipynb"], "titles": ["Enrichment of the Corpus", "Analysis of flu waves", "Corpus Creation", "Data Input and Homogenisation", "Waves of the Spanish Flu \u2013 Case Study", "Pre-Processing", "Analysis"], "terms": {"now": [0, 6], "we": [0, 4, 6], "have": 0, "homogen": [0, 2], "prepar": 0, "i": 0, "e": 0, "pre": [0, 4], "process": [0, 4], "more": 0, "depth": 0, "analys": 0, "thi": [0, 4, 6], "lemmat": 0, "tag": [0, 3], "can": 0, "done": 0, "multipl": 0, "what": 0, "concept": 0, "ar": [0, 3], "known": 0, "do": [0, 2, 6], "need": [0, 3], "notion": 0, "d": [0, 6], "sai": 0, "ye": 0, "spaci": [0, 5], "stanford": 0, "core": 0, "nltk": 0, "languag": 0, "specif": [0, 5], "pathlib": 0, "import": [0, 3], "replac": [0, 3], "point": 0, "standard": 0, "here": [0, 2], "corpus_dir": 0, "r": [0, 6], "collect": [0, 2], "ordereddict": 0, "def": [0, 3, 6], "read_corpus_linewis": 0, "str": [0, 3, 6], "filepath": 0, "iterdir": 0, "is_fil": 0, "read_text": 0, "n": [0, 6], "name": [0, 5], "return": [0, 3, 6], "pr\u00fcfen": 0, "viel": [0, 3], "dateien": 0, "wurden": 0, "eingelesen": 0, "print": [0, 3, 6], "len": [0, 3, 6], "103": 0, "panda": [0, 6], "pd": [0, 6], "metadata_dir": 0, "metadata_filepath": 0, "mvp": 0, "test": 0, "korpus_metadata": 0, "csv": 0, "metadata_df": 0, "read_csv": [0, 6], "sep": 0, "sehen": 0, "unser": 0, "au": 0, "head": [0, 3], "newspap": [0, 4], "identifi": [0, 2], "date": 0, "link": 0, "0": [0, 3], "vossisch": 0, "zeitung": [0, 3], "snp27112366": 0, "19180101": 0, "http": 0, "content": 0, "staatsbibliothek": 0, "berlin": [0, 3, 4], "de": 0, "zef": 0, "19180108": 0, "19180115": 0, "19180122": 0, "19180129": 0, "all_text": 0, "join": [0, 3, 6], "valu": [0, 6], "word": [0, 6], "split": 0, "sieht": 0, "die": [0, 3], "wortlist": 0, "50": 0, "60": 0, "scarp": 0, "veiver\u017feit": 0, "von": 0, "me": 0, "und": [0, 3], "n\u00f6rdlich": 0, "der": [0, 3], "somm": 0, "hef": 0, "ntige": 0, "k\u00e4mpfe": 0, "w\u00f6rter": 0, "gibt": [0, 3], "insgesamt": 0, "1783703": 0, "welch": 0, "kommen": 0, "oft": 0, "vor": 0, "counter": 0, "word_frequ": 0, "chosen_word": 0, "input": [0, 4, 5, 6], "geben": 0, "sie": 0, "ein": [0, 3], "wort": 0, "f\u00fcr": [0, 3], "h\u00e4ufigkeit": 0, "angezeigt": 0, "wird": [0, 3], "stdinnotimplementederror": [0, 3, 6], "traceback": [0, 3, 6], "most": [0, 3, 6], "recent": [0, 3, 6], "call": [0, 3, 6], "last": [0, 3, 6], "cell": [0, 3, 6], "In": [0, 3, 4, 6], "18": 0, "line": [0, 3, 6], "virtualenv": [0, 3, 6], "fall": [0, 3, 6], "lib": [0, 3, 6], "python3": [0, 3, 6], "11": [0, 3, 6], "site": [0, 3, 6], "packag": [0, 3, 6], "ipykernel": [0, 3, 6], "kernelbas": [0, 3, 6], "py": [0, 3, 6], "1281": [0, 3, 6], "kernel": [0, 3, 6], "raw_input": [0, 3, 6], "self": [0, 3, 6], "prompt": [0, 3, 6], "1279": [0, 3, 6], "_allow_stdin": [0, 3, 6], "1280": [0, 3, 6], "msg": [0, 3, 6], "wa": [0, 3, 6], "frontend": [0, 3, 6], "doe": [0, 6], "support": [0, 3, 6], "request": [0, 3, 6], "rais": [0, 3, 6], "1282": [0, 3, 6], "_input_request": [0, 3, 6], "1283": [0, 3, 6], "1284": [0, 3, 6], "_parent_id": [0, 3, 6], "shell": [0, 3, 6], "1285": [0, 3, 6], "get_par": [0, 3, 6], "1286": [0, 3, 6], "password": [0, 3, 6], "fals": [0, 3, 6], "1287": [0, 3, 6], "instal": [0, 3], "first": 0, "probabl": 0, "want": 0, "show": 0, "how": 0, "would": 0, "overview": 0, "model": [0, 3], "avail": 0, "select": 0, "german": [0, 3], "de_core_news_sm": 0, "english": 0, "en_core_news_sm": 0, "python": [0, 3, 5], "m": 0, "download": 0, "disable_compon": 0, "ner": 0, "morpholog": 0, "attribute_rul": 0, "time": 0, "took_per_text": 0, "corpus_annot": [0, 6], "filename_list": 0, "list": 0, "kei": 0, "current": 0, "doc": 0, "enumer": 0, "pipe": 0, "disabl": 0, "befor": [0, 3], "append": [0, 6], "annotated_text": [0, 6], "tok": 0, "lemma_": 0, "tag_": 0, "sentenc": [0, 5], "sentence_idx": 0, "is_sent_start": 0, "datafram": [0, 6], "numpi": 0, "np": 0, "durschnittlich": 0, "pro": 0, "sekunden": 0, "mean": 0, "all": [0, 2], "zusammen": 0, "sum": 0, "l\u00e4nge": 0, "annotierten": 0, "gleich": 0, "originalkorpu": 0, "all_words_token": 0, "words_tokenized_frequ": 0, "anzahl": 0, "lemmata": 0, "uniqu": 0, "s\u00e4tze": 0, "satzl\u00e4ng": 0, "collected_metadata_extens": 0, "filenam": [0, 3, 6], "item": [0, 6], "metadata_extens": 0, "lemma_count": 0, "lemma_count_uniqu": 0, "sentence_count": 0, "iloc": [0, 6], "sentence_length_avg": 0, "groupbi": [0, 6], "count": [0, 6], "metadata_to_extend": 0, "astyp": 0, "metadata_extendend_df": 0, "merg": 0, "result_dir": 0, "conll": [0, 5, 6], "output_path": 0, "with_suffix": 0, "to_csv": 0, "index": [0, 6], "metadata_extended_filenam": 0, "v02": 0, "from": [2, 4], "pdf": 2, "jpg": [2, 3], "xml": 2, "txt": [2, 5], "ocr": 2, "tesseract": 2, "data": [2, 4], "sort": 2, "appli": 2, "extract": [2, 5], "text": [2, 5], "noth": 2, "file": [2, 3, 5, 6], "textual": 3, "might": 3, "come": 3, "It": 3, "could": 3, "gripp": [3, 6], "w\u00fctet": 3, "weiter": 3, "zunahm": 3, "schweren": 3, "f\u00e4lle": 3, "zahl": 3, "grippef\u00e4l": 3, "ist": 3, "den": 3, "letzten": 3, "beiden": 3, "tagen": 3, "auch": 3, "gro\u00df": 3, "noch": 3, "deutlich": 3, "gestiegen": 3, "warenh\u00e4us": 3, "sonstigen": 3, "gesch\u00e4ft": 3, "krieg": 3, "privaten": 3, "betrieb": 3, "klagen": 3, "dass": 3, "\u00fcberm\u00e4\u00dfig": 3, "angestellt": 3, "krank": 3, "melden": 3, "m\u00fcssen": 3, "bei": 3, "stra\u00dfenbahn": 3, "grippekranken": 3, "bedeutend": 3, "etc": 3, "morgenpost": 3, "octob": 3, "15": 3, "1918": [3, 4], "some": [3, 6], "html": 3, "p": 3, "To": 3, "digit": 3, "an": 3, "optic": 3, "charact": 3, "recognit": 3, "tool": 3, "There": 3, "commerci": 3, "ones": 3, "like": [3, 6], "fineread": 3, "ll": [3, 6], "open": 3, "free": 3, "pip": 3, "pytesseract": 3, "pillow": 3, "pil": 3, "sampl": 3, "part": 3, "deutsch": 3, "ausgaben": 3, "am": 3, "montag": 3, "23": [3, 6], "12": 3, "u": [3, 6], "ocr_output": 3, "image_to_str": 3, "lang": 3, "frk": 3, "fraktur": 3, "lage": 3, "anfdemkohlenmarkt": 3, "zu": 3, "en": 3, "\u017fhlimm": 3, "\u017ften": 3, "bef\u00fcrc": 3, "tungen": 3, "anla\u00df": 3, "sach\u017fen": 3, "fehlten": 3, "im": 3, "nov": 3, "mber": 3, "30": 3, "000": 3, "wagen": 3, "je": 3, "10": [3, 6], "tonnen": 3, "tezembex": 3, "mit": 3, "gr\u00f6\u00dferen": 3, "ausf\u00e4llen": 3, "gere": 3, "net": 3, "werden": 3, "e3": 3, "i\u017ft": 3, "einem": 3, "v\u00f6lligen": 3, "still\u017ftand": 3, "indu\u017ftri": 3, "innerhalb": 3, "vierzehn": 3, "red": 3, "hnen": 3, "wenn": 3, "nicht": 3, "erheblich": 3, "steigerung": 3, "belen\u017f": 3, "aften": 3, "kot": 3, "bergwerk": 3, "oder": 3, "ihrer": 3, "zah": 3, "geiingt": 3, "\u017fteht": 3, "we\u017fentlich": 3, "erh\u00f6hung": 3, "kohlenpreij": 3, "bevor": 3, "ground_truth": 3, "pleas": 3, "insert": [3, 6], "string": 3, "6": [3, 6], "levenshtein": 3, "lev": 3, "measure_qu": 3, "calcul": 3, "f1": 3, "score": 3, "distanc": 3, "align": 3, "param": 3, "A": 3, "contain": [3, 6], "result": [3, 6], "verifi": 3, "matching_part": 3, "matching_block": 3, "editop": 3, "true_po": 3, "x": 3, "f_score": 3, "round": 3, "4": 3, "nrecal": 3, "nf1": 3, "grippe1": 3, "png": 3, "gro\u00dfen": 3, "gewachsen": 3, "The": 3, "wai": 3, "chang": 3, "quickli": 3, "year": [3, 5, 6], "o": [3, 6], "tqdm": 3, "pdf2imag": 3, "convert_from_path": 3, "pathpdf": 3, "listdir": [3, 6], "thispath": 3, "path": [3, 6], "converted_pdf": 3, "use_cropbox": 3, "true": 3, "w": 3, "output_txt": 3, "recogn": 3, "write": [3, 5], "unlik": 3, "alreadi": 3, "machin": 3, "readabl": 3, "so": 3, "thei": 3, "lower": 3, "hang": 3, "fruit": 3, "still": 3, "parser": 3, "rid": 3, "metadata": [3, 6], "bs4": 3, "beautifulsoup": 3, "pathtoxmlfil": 3, "path2fil": 3, "openxml": 3, "soup": 3, "find": 3, "strip": 3, "output_xml": 3, "output": [3, 5], "explor": 4, "publish": 4, "brandenburg": 4, "1920": 4, "respect": 4, "report": 4, "corpu": [4, 6], "creation": 4, "homogenis": 4, "enrich": 4, "analysi": 4, "annot": 5, "token": [5, 6], "lemma": 5, "po": 5, "type": [5, 6], "sourc": 5, "length": 5, "vocabulari": 5, "size": 5, "librari": 5, "read": 5, "extractt": 5, "extend": 5, "take": 6, "look": 6, "analyz": 6, "re": 6, "conllfil": 6, "class": 6, "wordsearchengin": 6, "__init__": 6, "prepare_index_dataframe_for_search": 6, "month": 6, "dai": 6, "get_date_fnam": 6, "full_df": 6, "concat": 6, "f": 6, "shape": 6, "occur": 6, "redo": 6, "WITH": 6, "date_pattern": 6, "19": 6, "group": 6, "3": 6, "search_and_plot": 6, "search_term": 6, "queri": 6, "titl": 6, "engin": 6, "3008370": 6, "8": 6, "21": 6, "20": 6, "22": 6, "let": 6, "which": 6, "appear": 6, "contextview": 6, "reset_index": 6, "get_context": 6, "search_lemma": 6, "indic": 6, "left_context": 6, "this_word": 6, "right_context": 6, "left": 6, "left\u0441": 6, "right": 6, "right\u0441": 6, "newdf": 6, "sort_valu": 6}, "objects": {}, "objtypes": {}, "objnames": {}, "titleterms": {"enrich": 0, "corpu": [0, 2, 3], "1": [0, 3, 6], "read": 0, "txt": [0, 3], "data": [0, 3, 5, 6], "set": 0, "path": 0, "directori": 0, "2": [0, 3, 6], "file": 0, "from": [0, 3], "3": [0, 3], "metadata": [0, 5], "worth\u00e4ufigkeit": 0, "mit": 0, "lazi": 0, "token": 0, "load": 0, "nlp": 0, "librari": 0, "up": 0, "pipelin": 0, "annot": 0, "text": [0, 3], "extract": 0, "lemma": [0, 6], "po": 0, "wie": 0, "lang": 0, "hat": 0, "da": 0, "annotieren": 0, "gedauert": 0, "4": 0, "echter": 0, "metadaten": 0, "ausweiten": 0, "sammeln": 0, "hinzuf\u00fcgen": 0, "5": 0, "ergebniss": 0, "speichern": 0, "annotiert": 0, "korpu": 0, "erweitert": 0, "analysi": [1, 3, 6], "flu": [1, 4], "wave": [1, 4], "creation": 2, "task": 2, "descript": [2, 5], "differ": [2, 3], "format": [2, 3], "specif": 2, "tool": [2, 5], "approach": [2, 5], "result": 2, "input": 3, "homogenis": 3, "type": 3, "corpora": 3, "we": 3, "have": 3, "abl": 3, "us": 3, "all": 3, "sourc": 3, "unifi": 3, "imag": 3, "digial": 3, "ocr": 3, "evalu": 3, "engin": 3, "qualiti": 3, "manual": 3, "creat": 3, "ground": 3, "truth": 3, "against": 3, "measur": 3, "precis": 3, "recal": 3, "f": 3, "sidenot": 3, "gpt4": 3, "doe": 3, "pretti": 3, "good": 3, "job": 3, "post": 3, "correct": 3, "raw": 3, "ed": 3, "\u0441leaner": 3, "perhap": 3, "shouldn": 3, "t": 3, "encourag": 3, "student": 3, "do": 3, "thi": 3, "point": 3, "But": 3, "": 3, "awar": 3, "process": [3, 5], "whole": 3, "pdf": 3, "same": 3, "after": 3, "run": 3, "our": 3, "plain": 3, "form": 3, "get": 3, "structur": 3, "markup": 3, "xml": 3, "now": 3, "let": 3, "next": 3, "notebook": 3, "spanish": 4, "case": 4, "studi": 4, "pre": 5, "more": 5, "linguist": 5, "concept": 5, "0": 6, "import": 6, "upload": 6, "search": 6, "plot": 6, "frequenc": 6, "explor": 6, "context": 6, "kwic": 6, "colloc": 6}, "envversion": {"sphinx.domains.c": 3, "sphinx.domains.changeset": 1, "sphinx.domains.citation": 1, "sphinx.domains.cpp": 9, "sphinx.domains.index": 1, "sphinx.domains.javascript": 3, "sphinx.domains.math": 2, "sphinx.domains.python": 4, "sphinx.domains.rst": 2, "sphinx.domains.std": 2, "sphinx.ext.intersphinx": 1, "sphinxcontrib.bibtex": 9, "sphinx": 60}, "alltitles": {"Enrichment of the Corpus": [[0, "enrichment-of-the-corpus"]], "1. Read in the txt data": [[0, "read-in-the-txt-data"]], "1.1 Set path to corpus directory": [[0, "set-path-to-corpus-directory"]], "1.2 Read in the files from the directory": [[0, "read-in-the-files-from-the-directory"]], "1.3 Read in metadata": [[0, "read-in-metadata"]], "2.Worth\u00e4ufigkeit mit lazy tokenization": [[0, "worthaufigkeit-mit-lazy-tokenization"]], "3. Load NLP Library": [[0, "load-nlp-library"]], "3.1 Load library": [[0, "load-library"]], "3.2 Setting up the pipeline": [[0, "setting-up-the-pipeline"]], "3.3 Annotate texts and extract token, lemma, pos": [[0, "annotate-texts-and-extract-token-lemma-pos"]], "Wie lange hat das Annotieren gedauert?": [[0, "wie-lange-hat-das-annotieren-gedauert"]], "3.4 Worth\u00e4ufigkeit mit echter Tokenization": [[0, "worthaufigkeit-mit-echter-tokenization"]], "4. Metadaten ausweiten": [[0, "metadaten-ausweiten"]], "4.1 Metadaten sammeln": [[0, "metadaten-sammeln"]], "4.2 Metadaten hinzuf\u00fcgen": [[0, "metadaten-hinzufugen"]], "5. Ergebnisse speichern": [[0, "ergebnisse-speichern"]], "5.1 Annotiertes Korpus speichern": [[0, "annotiertes-korpus-speichern"]], "5.2 Erweiterte Metadaten speichern": [[0, "erweiterte-metadaten-speichern"]], "Analysis of flu waves": [[1, "analysis-of-flu-waves"]], "Corpus Creation": [[2, "corpus-creation"]], "Task Description": [[2, "task-description"]], "Different formats and their specifics": [[2, "different-formats-and-their-specifics"]], "Tool": [[2, "tool"]], "Approach": [[2, "approach"], [5, "approach"]], "Result": [[2, "result"]], "Data Input and Homogenisation": [[3, "data-input-and-homogenisation"]], "1. Types of input data for text corpora": [[3, "types-of-input-data-for-text-corpora"]], "We have to be able to use all these formats and homogenise different sources into a unified corpus": [[3, "we-have-to-be-able-to-use-all-these-formats-and-homogenise-different-sources-into-a-unified-corpus"]], "2.  images into digial text. OCR": [[3, "images-into-digial-text-ocr"]], "2.1. Evaluate OCR engine quality": [[3, "evaluate-ocr-engine-quality"]], "2.1.1 Manually create  the \u2018ground truth\u2019 to evaluate against": [[3, "manually-create-the-ground-truth-to-evaluate-against"]], "2.1.2 Measure OCR precision, recall and F-measure": [[3, "measure-ocr-precision-recall-and-f-measure"]], "Sidenote: GPT4 does a pretty good job of OCR post-correction:": [[3, "sidenote-gpt4-does-a-pretty-good-job-of-ocr-post-correction"]], "Image:": [[3, "image"]], "Raw OCR-ed text": [[3, "raw-ocr-ed-text"]], "post-correction with GPT4:": [[3, "post-correction-with-gpt4"]], "\u0421leaner text:": [[3, "leaner-text"]], "Perhaps we shouldn\u2019t encourage students to do this at this point\u2026 But it\u2019s good to be aware of this.": [[3, "perhaps-we-shouldn-t-encourage-students-to-do-this-at-this-point-but-it-s-good-to-be-aware-of-this"]], "2.2 Process the whole corpus of PDF-s with the same OCR engine": [[3, "process-the-whole-corpus-of-pdf-s-with-the-same-ocr-engine"]], "After running this we have all our PDF-s in plain txt form": [[3, "after-running-this-we-have-all-our-pdf-s-in-plain-txt-form"]], "3.  Getting digial text from the structured markup (XML)": [[3, "getting-digial-text-from-the-structured-markup-xml"]], "After running this we have all our XML-s in plain txt form": [[3, "after-running-this-we-have-all-our-xml-s-in-plain-txt-form"]], "Now let\u2019s use all the data for processing and analysis (next notebook)": [[3, "now-let-s-use-all-the-data-for-processing-and-analysis-next-notebook"]], "Waves of the Spanish Flu \u2013 Case Study": [[4, "waves-of-the-spanish-flu-case-study"]], "Pre-Processing": [[5, "pre-processing"]], "Description": [[5, "description"]], "More to linguistic concepts": [[5, "more-to-linguistic-concepts"]], "Data": [[5, "data"]], "Metadata": [[5, "metadata"]], "Tools": [[5, "tools"]], "Analysis": [[6, "analysis"]], "0. Imports and data upload": [[6, "imports-and-data-upload"]], "1. Search lemma and plot frequency": [[6, "search-lemma-and-plot-frequency"]], "2. Exploring the contexts": [[6, "exploring-the-contexts"]], "2.1 KWIC": [[6, "kwic"]], "2.2 Collocations": [[6, "collocations"]]}, "indexentries": {}})