{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d59fe07",
   "metadata": {},
   "source": [
    "# Data Input and Homogenisation\n",
    "\n",
    "## 1. Types of input data for text corpora\n",
    "\n",
    "Textual data might come in different forms. \n",
    "\n",
    "1. It could be **plain text**:\n",
    "\n",
    "```\n",
    "Die Grippe wütet weiter\n",
    "Zunahme der schweren Fälle in Berlin. Die Zahl der Grippefälle ist in den letzten beiden Tagen auch in Groß-Berlin noch deutlich gestiegen. Die Warenhäuser und sonstigen Geschäfte, die Kriegs- und die privaten Betriebe klagen, dass übermäßig viele Angestellte krank melden müssen, und auch bei der Post und bei der Straßenbahn ist die Zahl der Grippekranken bedeutend gestiegen.\n",
    "```\n",
    "\n",
    "2. It could be **images** (pdf, jpg, etc):\n",
    "\n",
    "<img src=\"grippe1.png\" width=700>\n",
    "\n",
    "(source: Berliner Morgenpost, October 15, 1918)\n",
    "\n",
    "3. It could be some **structured markup** (XML/HTML):\n",
    "\n",
    "```\n",
    "<text>\n",
    "    <head>\n",
    "        Die Grippe wütet weiter\n",
    "    </head>\n",
    "    <p>\n",
    "        <s>Zunahme der schweren Fälle in Berlin.</s> \n",
    "        <s>Die Zahl der Grippefälle ist in den letzten beiden Tagen auch in Groß-Berlin noch deutlich gestiegen.</s>\n",
    "        <s>Die Warenhäuser und sonstigen Geschäfte, die Kriegs- und die privaten Betriebe klagen, dass übermäßig viele Angestellte krank melden müssen, und auch bei der Post und bei der Straßenbahn ist die Zahl der Grippekranken bedeutend gestiegen.</s>\n",
    "    </p>\n",
    "</text>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8aaad4b",
   "metadata": {},
   "source": [
    "#### We have to be able to use all these formats and homogenise different sources into a unified corpus "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29466219",
   "metadata": {},
   "source": [
    "## 2.  images into digial text. OCR\n",
    "\n",
    "To process images into digital text we need an **Optical Character Recognition (OCR)** tool. There are some commercial ones (like FineReader), we'll use an open & free tool in Python. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af423bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pytesseract\n",
    "#!pip install pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a636708",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273defc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9efa76",
   "metadata": {},
   "source": [
    "### 2.1. Evaluate OCR engine quality\n",
    "\n",
    "Before processing all files, we evaluate OCR quality on a sample image part for evaluation(source: [Deutsche Zeitung, Ausgaben am Montag, 23.12.1918](https://zefys.staatsbibliothek-berlin.de/kalender/auswahl/date/1918-12-23/30744015/)):\n",
    "![sample.jpg](sample.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73d817e",
   "metadata": {},
   "source": [
    "Let us OCR it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c9fd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "ocr_output = pytesseract.image_to_string(Image.open('sample.jpg'), lang='frk')  # using German fraktur OCR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ca0449",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ocr_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1328b4ad",
   "metadata": {},
   "source": [
    "#### 2.1.1 Manually create  the 'ground truth' to evaluate against"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a68c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = input('Please insert corrected string: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c35c218",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ground_truth)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc90f22d",
   "metadata": {},
   "source": [
    "#### 2.1.2 Measure OCR precision, recall and F-measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064eddee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Levenshtein as lev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b306d511",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_quality(ocr_output, ground_truth):\n",
    "    \"\"\"\n",
    "    Calculates precision, recall, and F1-score\n",
    "    using the Levenshtein distance to align text from OCR with the ground truth data.\n",
    "\n",
    "    :param ocr_output: A string containing the raw OCR results.\n",
    "    :param ground_truth: A string containing the verified ground truth text.\n",
    "    \"\"\"\n",
    "\n",
    "    matching_parts = lev.matching_blocks(lev.editops(ocr_output, ground_truth), ocr_output, ground_truth)\n",
    "    true_pos = len(''.join([ocr_output[x[0]:x[0]+x[2]] for x in matching_parts]))\n",
    "\n",
    "    precision = true_pos / len(ground_truth)\n",
    "    recall = true_pos / len(ocr_output)\n",
    "    f_score = 2 * ((precision * recall) / (precision + recall))\n",
    "\n",
    "    return precision, recall, f_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03132c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, f_score = measure_quality(ocr_output, ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46857ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Precision: {round(precision, 4)}\\nRecall: {round(recall, 4)}\\nF1-score: {round(f_score, 4)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81072532",
   "metadata": {},
   "source": [
    "## Sidenote: GPT4 does a pretty good job of OCR post-correction:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a142cf",
   "metadata": {},
   "source": [
    "### Image:\n",
    "\n",
    "<img src=\"grippe1.png\" width=500>\n",
    "\n",
    "### Raw OCR-ed text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04dd295b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ocr_output = pytesseract.image_to_string(Image.open('grippe1.png'), lang='frk') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91b85cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ocr_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7dd2a1",
   "metadata": {},
   "source": [
    "### post-correction with GPT4:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa69d70a",
   "metadata": {},
   "source": [
    "<img src=\"gpt_postcorr.png\" width=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b028819",
   "metadata": {},
   "source": [
    "### Сleaner text:\n",
    "\n",
    "Grippe wütet weiter\n",
    "\n",
    "Zunahme der schweren Fälle in Berlin.\n",
    "\n",
    "Die Zahl der Grippefälle ist in den letzten beiden Tagen auch in Groß-Berlin noch erheblich gestiegen. Die Warenhäuser und sonstigen großen Geschäfte, die Kriegs- und die privaten Betriebe klagen, dass übermäßig viele Angestellte krank melden müssen, und auch bei der Post und bei der Straßenbahn ist die Zahl der Grippekranken bedeutend gewachsen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0011d1",
   "metadata": {},
   "source": [
    "#### Perhaps we shouldn't encourage students to do this at this point... But it's good to be aware of this.\n",
    "The way we do OCR might change quickly in the coming years"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c60c71",
   "metadata": {},
   "source": [
    "### 2.2 Process the whole corpus of PDF-s with the same OCR engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74682bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "from pdf2image import convert_from_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91568303",
   "metadata": {},
   "outputs": [],
   "source": [
    "pathpdf = '../data/pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a69f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in tqdm(os.listdir(pathpdf)):\n",
    "    if '.pdf' in filename:\n",
    "        thispath = os.path.join(pathpdf, filename)\n",
    "        converted_pdf = convert_from_path(thispath, use_cropbox=True)\n",
    "        with open(thispath.replace('.pdf', '.txt'), 'w') as output_txt:\n",
    "            for image in converted_pdf:\n",
    "                recognized = pytesseract.image_to_string(image, \n",
    "                                                         lang='frk') \n",
    "                output_txt.write(recognized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c652166",
   "metadata": {},
   "source": [
    "#### After running this we have all our PDF-s in plain txt form"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae4d69d",
   "metadata": {},
   "source": [
    "## 3.  Getting digial text from the structured markup (XML)\n",
    "\n",
    "Unlike text on the image, XML/HTML are already machine readable, so they are a lower-hanging fruit. Still, we'll need to use a parser for such markup to get rid of XML/HTML tags and some metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1538a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1237d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pathtoxmlfiles = '../data/xml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272de86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir(pathtoxmlfiles):\n",
    "    if '.xml' in filename:\n",
    "        path2file = os.path.join(pathtoxmlfiles, filename)\n",
    "        with open(path2file) as openxml:\n",
    "            soup = BeautifulSoup(openxml)\n",
    "        print(soup.find('text').text.strip())\n",
    "        #with open(path2file.replace('.txt', '.xml'), 'w') as output_xml:\n",
    "        #    output.write(soup.find('text').text.strip())\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43785012",
   "metadata": {},
   "source": [
    "#### After running this we have all our XML-s in plain txt form"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f888d4",
   "metadata": {},
   "source": [
    "## Now let's use all the data for processing and analysis (next notebook)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
